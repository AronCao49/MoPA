import argparse
import numpy as np
import torch
import cv2
import os
import pickle
import warnings
from tqdm import tqdm
import matplotlib.pyplot as plt
from typing import Any, Dict, List, Optional, Tuple
from torchvision.ops.boxes import batched_nms, box_area

from onnxruntime.quantization import QuantType
from onnxruntime.quantization.quantize import quantize_dynamic

from segment_anything import (
    sam_model_registry, 
    SamAutomaticMaskGenerator
    )
from segment_anything.utils.onnx import SamOnnxModel


# from SAM example code
# https://github.com/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb
def show_mask(mask, ax):
    color = np.array([30/255, 144/255, 255/255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)
    
def show_points(coords, labels, ax, marker_size=375):
    pos_points = coords[labels==1]
    neg_points = coords[labels==0]
    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)
    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   
    
def show_box(box, ax):
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]
    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))  
    
def export_sam_onnx(
    model_type: str,
    ckpt_pth: str,
    onnx_pth: str,
    onnx_qt_pth: str
):
    """Function to convert SAM checkpoint to onnx model

    Args:
        model_type (str): the type of the model
        ckpt_pth (str): path to the SAM checkpoint
        onnx_pth (str): path to the saved ONNX model
        onnx_qt_pth (str): path to the quantized ONNX model
    """
    
    # Load model from the checkpoint
    sam = sam_model_registry[model_type](checkpoint=ckpt_pth)
    onnx_model = SamOnnxModel(sam, return_single_mask=True)

    dynamic_axes = {
        "point_coords": {1: "num_points"},
        "point_labels": {1: "num_points"},
    }

    embed_dim = sam.prompt_encoder.embed_dim
    embed_size = sam.prompt_encoder.image_embedding_size
    mask_input_size = [4 * x for x in embed_size]
    dummy_inputs = {
        "image_embeddings": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),
        "point_coords": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),
        "point_labels": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),
        "mask_input": torch.randn(1, 1, *mask_input_size, dtype=torch.float),
        "has_mask_input": torch.tensor([1], dtype=torch.float),
        "orig_im_size": torch.tensor([1500, 2250], dtype=torch.float),
    }
    output_names = ["masks", "iou_predictions", "low_res_masks"]

    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=torch.jit.TracerWarning)
        warnings.filterwarnings("ignore", category=UserWarning)
        with open(onnx_pth, "wb") as f:
            torch.onnx.export(
                onnx_model,
                tuple(dummy_inputs.values()),
                f,
                export_params=True,
                verbose=True,
                opset_version=12,
                do_constant_folding=True,
                input_names=list(dummy_inputs.keys()),
                output_names=output_names,
                dynamic_axes=dynamic_axes,
            )
    
    quantize_dynamic(
        model_input=onnx_pth,
        model_output=onnx_qt_pth,
        optimize_model=True,
        per_channel=False,
        reduce_range=False,
        weight_type=QuantType.QUInt8,
    )


def show_anns(anns: dict):
    """Simple visualization from SAM code

    Args:
        anns (dict): output mask generated by mask_generator
    """
    if len(anns) == 0:
        return
    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
    ax = plt.gca()
    ax.set_autoscale_on(False)

    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))
    img[:,:,3] = 0
    for ann in sorted_anns:
        m = ann['segmentation']
        color_mask = np.concatenate([np.random.random(3), [0.35]])
        img[m] = color_mask
    ax.imshow(img)


def single_mask_generator(img_pth, mask_pth, mask_generator):
    
    image = cv2.imread(img_pth)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # Generate masks
    masks = mask_generator.generate(image)
    
    # Concate all masks and save
    sorted_masks = sorted(masks, key=(lambda x: x['area']), reverse=True)
    masks = np.zeros_like(sorted_masks[0]['segmentation'], dtype=np.uint8)
    idx = 1
    for mask in sorted_masks:
        masks[mask['segmentation']] = idx
        idx += 1
    masks.tofile(mask_pth)


def images_mask_generator(
    image_dir_pth: str,
    mask_dir_pth: str,
    mask_generator: SamAutomaticMaskGenerator,
):
    """Simple function to save masks of images in the image_dir

    Args:
        sam_ckpt_pth (str): path to the SAM checkpoint file
        image_dir_pth (str): path to the image directory
        mask_dir_pth (str): path to the mask saving directory
        model_type (str): type of SAM model
    """
    
    for img_file in tqdm(sorted(os.listdir(image_dir_pth))):
        # Visualization
        # plt.figure(figsize=(20,20))
        # plt.imshow(image)
        # show_anns(masks)
        # plt.axis('off')
        # plt.savefig("mopa/samples/sam_sample.jpg")
        # input("Press Enter to continue......")
        
        img_pth = os.path.join(image_dir_pth, img_file)
        mask_pth = os.path.join(mask_dir_pth, img_file.replace(".png", ".bin"))
        single_mask_generator(img_pth, mask_pth, mask_generator)
        

def kitti_mask_generator(
    model_type: str,
    sam_ckpt_pth: str,
    img_dir_prefix: str,
    mask_dir_prefix: str,
):
    """Function to generate SAM masks for SemanticKITTI

    Args:
        model_type (str): Name of SAM model type.
        sam_ckpt_pth (str): Path to pretrained SAM model.
        img_dir_prefix (str): Prefix of SemanticKITTI image directory.
        mask_dir_prefix (str): Prefix of SAM mask saving directory.
    """
    sam = sam_model_registry[model_type](checkpoint=sam_ckpt_pth)
    sam.to(device="cuda")
    mask_generator = SamAutomaticMaskGenerator(sam)
    print("Load {} from {}".format(model_type.upper(), sam_ckpt_pth))
    
    train = [
        '00',
        '01',
        '02',
        '03',
        '04',
        '05',
        '06',
        '09',
        '10',
    ]
    
    for seq_dir in sorted(os.listdir(img_dir_prefix)):
        if seq_dir not in train:
            continue
        print("Infering sequence: {}".format(seq_dir))
        image_dir = os.path.join(img_dir_prefix, seq_dir, "image_2")
        mask_dir = os.path.join(mask_dir_prefix, seq_dir)
        if not os.path.exists(mask_dir):
            os.mkdir(mask_dir)
        images_mask_generator(image_dir, mask_dir, mask_generator)      

def nuscenes_mask_generator(
    model_type: str,
    sam_ckpt_pth: str,
    nuscene_dir_prefix: str,
    mask_dir_prefix: str,
    pickle_pth: str,
):
    """Function to generate SAM masks for SemanticKITTI

    Args:
        model_type (str): Name of SAM model type.
        sam_ckpt_pth (str): Path to pretrained SAM model.
        nuscene_dir_prefix (str): Path prefix to NuScenes dataset.
        mask_dir_prefix (str): Prefix of SAM mask saving directory.
        pickle_pth (str): Path to load training pickle.
    """
    sam = sam_model_registry[model_type](checkpoint=sam_ckpt_pth)
    sam.to(device="cuda")
    mask_generator = SamAutomaticMaskGenerator(sam)
    print("Load {} from {}".format(model_type.upper(), sam_ckpt_pth))
    
    data_list = []
    with open(pickle_pth, 'rb') as f:
        data_list.extend(pickle.load(f))
        
    for data_dict in tqdm(data_list):
        seq_dir, img_file = data_dict['camera_path'].split('/')[-2], \
            data_dict['camera_path'].split('/')[-1]
        img_pth = os.path.join(nuscene_dir_prefix, data_dict['camera_path'])
        mask_pth = os.path.join(mask_dir_prefix, seq_dir, img_file.replace(".jpg", ".bin"))
        
        if not os.path.exists(os.path.join(mask_dir_prefix, seq_dir)):
            os.makedirs(os.path.join(mask_dir_prefix, seq_dir))
        
        single_mask_generator(img_pth, mask_pth, mask_generator)


def parse_args():
    parser = argparse.ArgumentParser(description='SAM Mask Extraction')
    parser.add_argument('--task', default="MoPA", type=str, help='task name')
    parser.add_argument('--model_type',
                        required=True, 
                        choices=['vit-h', 'vit-l', 'vit-b'], 
                        type=str, help='type of SAM model')
    parser.add_argument('--sam_ckpt_pth',
                        required=True,
                        type=str, help='path to SAM model checkpoint')
    args = parser.parse_args()
    return args


if __name__ == "__main__":
    args = parse_args
    
    model_type = args.model_type
    sam_ckpt_pth = args.sam_ckpt_pth
    
    # KITTI Mask
    img_dir_prefix = "mopa/datasets/semantic_kitti/dataset/sequences"
    mask_dir_prefix = "mopa/datasets/semantic_kitti/img_mask"
    kitti_mask_generator(
        model_type, sam_ckpt_pth, img_dir_prefix, mask_dir_prefix
    )
    
    # NuScenes Mask
    img_dir_prefix = "mopa/datasets/nuscenes"
    mask_night_dir_prefix = "mopa/datasets/nuscenes/img_mask/train_night"
    mask_sg_dir_prefix = "mopa/datasets/nuscenes/img_mask/train_singapore"
    pickle_night_pth = "mopa/datasets/nuscenes/preprocess/train_night.pkl"
    pickle_sg_pth = "mopa/datasets/nuscenes/preprocess/train_singapore.pkl"
    nuscenes_mask_generator(
        model_type, sam_ckpt_pth, img_dir_prefix, mask_night_dir_prefix, pickle_night_pth
    )
    nuscenes_mask_generator(
        model_type, sam_ckpt_pth, img_dir_prefix, mask_sg_dir_prefix, pickle_sg_pth
    )